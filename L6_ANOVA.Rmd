---
title: "Statistical Methods for High Dimensional Biology<br><font size=8>STAT/BIOF/GSAT 540</font>"
subtitle: "<font color=red> Lecture 6 -- ANOVA and Linear Models</font> <br style=line-height:2> Keegan Korthauer"
date: "2020/01/20 <br><font size=5> Slides by: Gabriela Cohen Freue with contributions from Jenny Bryan </font>"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      highlightSpans: true
      countIncrementalSlides: false
      ratio: "16:9"
--- 
class: middle
layout: true
---

class: middle, center
### <font color="red"> Are these genes truly different in NrlKO compared to WT?</font>

### H<sub>0</sub>: the expression level of gene A is the same in both conditions.

### Is there **enough** evidence in the data to reject H<sub>0</sub>?

```{r, include=FALSE}
library(dplyr)
library(ggplot2)
library(ggthemes)
library(grid)
library(gridExtra)
library(lattice)
library(latticeExtra)

jCols <- c(x = "blue", y = "orange")
trellis.par.set(superpose.symbol = list(col = jCols),
                superpose.line = list(col = jCols))
jCex <- 3 

prDes <- readRDS("data/GSE4051_design.rds")
str(prDes)

prDat<-read.table("data/GSE4051_data.tsv",
                      sep = "\t", header = T, row.names = 1)
str(prDat, list.len = 10)

miniDat <- as.vector(t(prDat[c("1422248_at", "1450946_at"), ]))
miniDat <- data.frame(gene = rep(c("Irs4", "Nrl"), each = nrow(prDes)),
                      gExp = miniDat)
miniDat <- data.frame(prDes, miniDat) # ignore the warning about row names
miniDat$gType <- factor(miniDat$gType, rev(levels(miniDat$gType)))
str(miniDat)
```

```{r echo=FALSE, fig.height=3.5, dev='svg'}
irsDat <- filter(miniDat, gene == "Irs4")
nrlDat <- filter(miniDat, gene == "Nrl")

irsLim <- ggplot(irsDat, aes(x = gExp, y = gType, colour = gType)) + 
             geom_point(alpha = 0.5) +
             labs(title = "Irs4 Gene Expression") +
             theme_bw() +
             theme(legend.position = "none") +
             xlim(5, 15)

nrlLim <- ggplot(nrlDat, aes(x = gExp, y = gType, colour = gType)) + 
             geom_point(alpha = 0.5) +
             labs(title = "Nrl Gene Expression") +
             theme_bw() +
             theme(legend.position = "none") +
             xlim(5, 15)

options(repr.plot.width=8, repr.plot.height=5)

grid.arrange(irsLim, nrlLim, ncol = 1)
```

---
class: middle
### **Statistics**: use a random sample to learn about the population
![](L6_ANOVA_files/pop_sample.png)
---
class: middle

# Last class: hypothesis testing

### 1. Define a <font color = "red">test statistic</font> to test $H_0$
- 2-sample *t*-test
- Welch *t*-test
- Wilcoxon rank-sum test
- Kolmogorov-Smirnov test

### 2. Compute the <font color=red>observed value</font> for the test statistic
### 3. Compute the probability of seeing a test statistic as extreme as that observed, under the <font color = "red">null sampling distribution</font> (p-value) 
### 4. Make a decision about the <font color = "red">significance</font> of the results, based on a pre-specified value (alpha, significance level)
---
class: middle
## We can run these tests in R 
### Example: use the `t.test` function to test $H_0$ using a classical 2-sample *t*-test.

``` {r,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
miniDat %>% subset(gene=="Irs4")%>% t.test(gExp ~ gType, data=., var.equal = TRUE)
```
---
class: middle
# Today...

### - show how to compare means of different groups (2 or more) using a linear regression model

  - dummy variables to model the levels of a qualitative explanatory variable

### - write a linear model using matrix notation

  - understand which matrix is built by R
  
### - distinguish between conditional and marginal effects
  - $t$-tests vs $F$-tests  
---
class: middle
![](L6_ANOVA_files/t_aov_lm.png)

---
class: middle
It seems that we can use any of these methods to test $H_0$

![](L6_ANOVA_files/t_aov_lm_res.png)
---
class: middle
## *t*-test *vs* linear regression: <font color = "red">why the same results?</font>

``` {r}
irs4Dat <- subset(miniDat,gene=="Irs4")
ttest.irs4<-t.test(gExp ~ gType, irs4Dat, var.equal = TRUE)
list("t value"=ttest.irs4$stat,"p-value"=ttest.irs4$p.value)
```


```{r}
lm.irs4 <- summary(lm(gExp ~ gType, irs4Dat))
list("t value"=lm.irs4$coeff[2,3],"p-value"=lm.irs4$coeff[2,4])
```

---
class: middle
## *t*-test *vs* linear regression: <font color = "red">where's the line?</font>

```{r echo=FALSE, fig.width=5, fig.height= 3, dev='svg'}
irsDat <- filter(miniDat, gene == "Irs4")
nrlDat <- filter(miniDat, gene == "Nrl")

irsLim <- ggplot(irsDat, aes(x = gExp, y = gType, colour = gType)) + 
             geom_point(alpha = 0.5) +
             labs(title = "Irs4") +
             theme_bw() +           
             theme(legend.position = "none") +
             xlim(5, 15)

nrlLim <- ggplot(nrlDat, aes(x = gExp, y = gType, colour = gType)) + 
             geom_point(alpha = 0.5) +
             labs(title = "Nrl") +
             theme_bw() +
             theme(legend.position = "none") +
             xlim(5, 15)

grid.arrange(irsLim, nrlLim, ncol = 1)
```
> Note that the $y$-axis in these plots is not numerical, thus a line in this space does not have any mathematical meaning. 

### Why can we run a $t$-test with a <font color = "red">linear</font> regression model?
---
class: middle
# From *t*-test to linear regression
Let's change the notation to give a common framework to all methods
<br> <br>
<font size=5> $$Y \sim G; \; E[Y] = \mu_Y$$ </font>
 **<center>↓</center>**

<font size=5> $$Y = \mu_Y + \varepsilon_Y; \; \varepsilon_Y \sim G; \; E[\varepsilon_Y] = 0$$ </font>
<br>
We can use a subindeces to distinguish observations from each group, i.e., 

<font size=5> $$Y_{ij} = \mu_j + \varepsilon_{ij};\; \; \varepsilon_{ij} \sim G_j; \; \;E[\varepsilon_{ij}] = 0;$$ </font>
<br>
where $j = \textrm{\{wt, NrlKO}\}$ or $j=\textrm{\{1, 2}\}$ identifies the groups; 
and $i=1, \ldots, n_j$ identifies the observations within each group
<br>

 > For example: $Y_{11}$ is the first observation in group 1 or WT
 
---
class: middle

## The goal is to test 

### $$H_0 : \mu_1 = \mu_2$$

### using data from the model

<font size=5> $$Y_{ij} = \mu_j + \varepsilon_{ij};\; \; \varepsilon_{ij} \sim G; \; \;E[\varepsilon_{ij}] = 0;$$</font>
<br>
where $j = \textrm{\{wt, NrlKO}\}$ or $j=\textrm{\{1, 2}\}$; and $i=1, \ldots, n_j$.
<br>

> For simplicity, we assume a common distribution $G$ for all groups

### Note that the population means are given by $E[Y_{ij}] = \mu_j$, i.e., the model is written with a <font color = "red">cell-means</font> - $\mu_j$ - parametrization
---
class: middle

### Note that for each group, the <font color = "red">population</font> mean is given by 
### $$E[Y_{ij}] = \mu_j,$$ 

###A natural **estimator** of the population mean is the <font color = "red">**sample mean**</font>

### Classical hypothesis testing methods use the group sample means as estimators

See, for example, the `t.test` function in R:

```{r}
ttest.irs4$estimate
```
---
class: middle
### However, the `lm` function reports other estimates, <font color = "red">why?</font>  

```{r,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
(means.irs4<-as.data.frame(irs4Dat %>% group_by(gType) %>% summarize(meanGroups=mean(gExp,digits=6))))
```

```{r}
lm.irs4$coefficients[,1]
```
$\hspace{3em}$ **↓**
.pull-left[
`(Intercept)` is the <font color = "red">sample mean</font> of NrlKO group 
]
.pull-right[
but `gTypewt` is <font color = "red">not</font> the sample mean of the WT group
]
---
class: middle

### Parametrizations: which parameters we use to write the model?
<Big>
By default, the `lm` does not use the cell-means parametrization 
The goal is to *compare* the means, not to study each in isolation


From <font color=red>**cell-means**</font> - $\mu_j$:  $$Y_{ij} = \mu_j + \varepsilon_{ij};\; \; \varepsilon_{ij} \sim G; \; \;E[\varepsilon_{ij}] = 0;$$

**<center>↓</center>**

to <font color = "red">**reference-treatment effect**</font> - $(\theta,\tau_j)$: $$Y_{ij} = \theta+\tau_j + \varepsilon_{ij};\; \; \tau_1=0, \; \; \varepsilon_{ij} \sim G; \; \;E[\varepsilon_{ij}] = 0;$$

Note that for each group, the population mean is given by 
$$E[Y_{ij}] = \theta+\tau_j=\mu_j,$$
and $\tau_2=\mu_2-\mu_1=E[Y_{i2}] -E[Y_{i1}]$ *compares* the means
---

class: middle
## Relation between parametrizations

![](L6_ANOVA_files/param_2.png)
---
class: middle


### `lm` reports the sample mean of the <font color = "red">reference</font> group (NrlKO): $\hat\theta$

### and the <font color = "red">treatment effect</font>, i.e., difference between the sample means of both groups: $\hat\tau_2$
 
```{r,tidy=TRUE, tidy.opts=list(width.cutoff=35)}
lm.irs4$coefficients[,1]
data.frame(meanWT=means.irs4[1,2],
        meanDiff=diff(means.irs4$meanGroups))
```

---
class: middle
## We still haven't answered ... where's the line?? 

<Big> 
$$Y_{ij} = \theta+\tau_j + \varepsilon_{ij};\; \; \tau_1=0, \; \; \varepsilon_{ij} \sim G; \; \;E[\varepsilon_{ij}] = 0;$$

```{r,echo=FALSE,fig.width=7, fig.height= 4, fig.align='center'}
nrlLim
```
---
class: middle
# <font color="red">Dummy</font> variables
<Big>

Let's re-write our model using <font color="red">dummy</font> (or indicator) variables:

$$Y_{ij} = \theta+\tau_j + \varepsilon_{ij};\; \; \tau_1=0, \; \; \varepsilon_{ij} \sim G; \; \;E[\varepsilon_{ij}] = 0;$$
**<center>↓</center>**

$$Y_{ij} = \theta+\tau_2 \times x_{ij} + \varepsilon_{ij};\; \; x_{ij}=\bigg\{\begin{array}{l} 
1\text{ if } j=2\\
0 \text{ otherwise}\\
\end{array}$$

<br>

> Note that $Y_{i1} = \theta + \varepsilon_{i1}$, because $\tau_1=0$ and $x_{i1}=0$ 
> and $Y_{i2} = \theta + \tau_2+ \varepsilon_{i2}$, because $x_{i2}=1$ (for all $i$)

The second form is written as a <font color="red">linear</font> ( $y=a + bx +\varepsilon$ ) regression, with a special (<font color="red">dummy</font>) explanatory variable <font color="red"> $x_{ij}$ </font> 
---

class: middle
  
### Using a dummy variables to model our categorical variables `gtype` we can perform a  <font color = "red">2-sample *t*-test</font> with a linear model

$$Y_{ij} = \theta+\tau_2 \times x_{ij} + \varepsilon_{ij};\; \; x_{ij}=\bigg\{\begin{array}{l}
1 \text{ if } j=2\\
0 \text{ if } j=1\\
\end{array}$$


``` {r}
list("t value"=ttest.irs4$stat,"p-value"=ttest.irs4$p.value)

list("t value"=lm.irs4$coeff[2,3],"p-value"=lm.irs4$coeff[2,4])
```
---

class: middle
## Beyond 2-groups comparisons: difference of means
<center>
```{r,echo=FALSE,out.height="500px"}
knitr::include_graphics("L6_ANOVA_files/more_2_groups.png")
```
</center>
---
class: middle

  
### Dummy variables can be used to model one *or more* categorical variables with 2 *or more* levels!

### <font color = "red">2-sample *t*-test</font> using a linear model

$$Y_{ij} = \theta+\tau_2 \times x_{ij} + \varepsilon_{ij};\; \; x_{ij}=\bigg\{\begin{array}{l}
1 \text{ if } j=2\\
0 \text{ if } j=1\\
\end{array}$$

### <font color = "red">1-way ANOVA with many levels</font> $^{(*)}$ using a linear model
$$Y_{ij} = \theta+\tau_2 \times x_{ij2} + \tau_3 \times x_{ij3} +\varepsilon_{ij};\; \; x_{ij2}=\bigg\{\begin{array}{l}
1\text{ if } j=2\\
0 \text{ otherwise}\\
\end{array}; \; x_{ij3}=\bigg\{\begin{array}{l}
0\text{ if } j=3\\
1 \text{ otherwise}\\
\end{array}$$

### This is why R can estimate all of them with `lm()`
<small>
$^{(*)}$ in general, *yet* another parametrization is used to present ANOVA 
---
class: middle

## <font color = "red">t-test</font> 
  > Special case of <font color = "red">ANOVA</font>, but with ANOVA you can compare **more than two groups** and **more than one factor**.
    
## <font color = "red">ANOVA</font> 

  > Special case of <font color = "red">linear regression</font>, but with linear regression you can include **quantitative variables** in the model. 
  
## <font color = "red">Linear regression</font> 

  > Provides a unifying framework to model the association between a response **many quantitative and qualitative variables**. 
 
### <font color = "red">In R</font>: all can be computed using the `lm()` function. 
---
class: middle
# Linear models using matrix notation

![](L6_ANOVA_files/linear_form.png)

### It will become handy to write our model using matrix notation
---
class: middle
<Big>

Let's form an $X$  matrix for a 3-groups comparison: 

$$Y_{ij} = \theta+\tau_2 \times x_{ij2} + \tau_3 \times x_{ij3} +\varepsilon_{ij}$$
<center>
```{r,echo=FALSE,out.height="400px"}
knitr::include_graphics("L6_ANOVA_files/model_matrix_I.png")
```
</center>

<font size=4>
Note that $x_{ij2}$ and $x_{ij3}$ become the 2nd and 3rd columns of $X$: $x_{i12}=x_{i13}=0$ for the reference group; $x_{i22}=1$ for the 2nd group; and $x_{i33}=1$ for the 3rd group

---
class: middle

```{r,echo=FALSE,out.height="350px"}
knitr::include_graphics("L6_ANOVA_files/model_matrix_II.png")
```

> <font color = "red">Note that $Y_{i1}= 1 \times \theta + 0 \times \tau_2 + 0 \times \tau_3 + \varepsilon_{i1}=\theta + \varepsilon_{i1}$

> <font color = "blue">Note that $Y_{i2}= 1 \times \theta + 1 \times \tau_2 + 0 \times \tau_3 + \varepsilon_{i2}=\theta + \tau_2+\varepsilon_{i2}$

> <font color = "green">Note that $Y_{i3}= 1 \times \theta + 0 \times \tau_2 + 1 \times \tau_3 + \varepsilon_{i3}=\theta + \tau_3+\varepsilon_{i3}$


<font color="black"> 
Which is the same as $\; \rightarrow \; Y_{ij} = \theta + \tau_j + \varepsilon_{ij}, \; \tau_1=0$

Which is the same as $\; \rightarrow \; Y_{ij} = \theta +\tau_2 \times x_{ij2} + \tau_3 \times x_{ij3} + \varepsilon_{ij}$

---
class: middle

```{r,echo=FALSE,out.height="320px"}
knitr::include_graphics("L6_ANOVA_files/rf_tx_matrix.png")
```

### Note that the model is still written with a reference-treatment parametrization (difference of means)


$E[Y_{i1}]=\theta$
<br>

$E[Y_{i2}]=\theta+\tau_2 \; \rightarrow \tau_2=E[Y_{i2}]-E[Y_{i1}]=\mu_2-\mu_1$
<br>

$E[Y_{i3}]=\theta+\tau_3 \; \rightarrow \tau_3=E[Y_{i3}]-E[Y_{i1}]=\mu_3-\mu_1$

---
class: middle

### <font color = "red">Linear regression</font> can include **quantitative & qualitative covariates**. 

<center>
```{r,echo=FALSE,out.height="470px"}
knitr::include_graphics("L6_ANOVA_files/LM_vbles.png")
```
</center>
> <font color="red">Linear</font> in the parameters $\alpha$: $X$ can contain $x^2$, $log(x)$, etc.

---
class: middle, center

## How it works in practice using <font color=red>lm()</font> in R

<big>

## $$Y = X\alpha + \varepsilon$$ 
**↓**
<br>

```
lm(y ~ x, data = yourData)
```
</font>
.pull-left[
<font color = red> y ~ x: </font> formula, 
<br><font color = red>y</font> numeric, 
<br><font color = red>x</font> numeric and/or factor
]

.pull-right[
<font color = red> yourData: </font> data.frame in which x and y are to be found (optional but recommended)
]
<br>
### By default, R uses a ref-tx parametrization but you can control that!
---
class: middle

## $$Y=X\alpha+\varepsilon$$
- ### Mathematically, $X$ is a numeric matrix
- ### If your data contains categorical variables (e.g., `gType`), you need to set them as **factors**
- ### R creates appropriate dummy variables for factors!

```{r}
str(irs4Dat$gType)
```
---
class: middle

### Under the hood, R creates a numeric $X$:

```{r,tidy=TRUE, tidy.opts=list(width.cutoff=45)}
data.frame(X=model.matrix(gExp ~ gType, irs4Dat),
                gType=irs4Dat$gType) %>% head(10)
```

---
class: middle
## Beyond 2-group comparisons in our case study:

### <font color="red"> Is the expression of gene A the same at all developmental stages?</font>

$$H_0 : \mu_{E16} = \mu_{P2} = \mu_{P6} = \mu_{P10} = \mu_{4W}$$
<center>
```{r, include=FALSE}
library(lattice)

prDes <- readRDS("data/GSE4051_design.rds")

prDat<-read.table("data/GSE4051_data.tsv",
                      sep = "\t", header = T, row.names = 1)

## I've selected this as our hit
theHit <- which(rownames(prDat) == "1440645_at") # 17843
## and this as our boring gene
theBore <- which(rownames(prDat) == "1443184_at") # 18898

keepers <- data.frame(row = c(theBore, theHit),
                       probesetID = I(rownames(prDat)[c(theBore, theHit)]))

devDat <- as.vector(t(prDat[keepers$probesetID, ]))
devDat <- data.frame(gene = rep(c("theBore", "theHit"), each = nrow(prDes)),gExp = devDat)
devDat <- data.frame(prDes, devDat)

boreDat <- filter(devDat, gene == "theBore")
hitDat <- filter(devDat, gene == "theHit")
```

```{r, echo=FALSE, fig.height= 3, dev='svg'}
boreLim <- ggplot(boreDat, aes(x = devStage, y = gExp)) + 
             geom_jitter(width = 0.2, alpha = 0.5) +
             labs(title = "theBore") +
             theme_bw() +
             theme(legend.position = "none") +
             ylim(5, 10) +
             xlab("") +
             stat_summary(aes(group=1), fun.y=mean, geom="line", colour="red")

hitLim <- ggplot(hitDat, aes(x = devStage, y = gExp)) + 
             geom_jitter(width = 0.2, alpha = 0.5) +
             labs(title = "theHit") +
             theme_bw() +
             theme(legend.position = "none") +
             ylim(5, 10) +
             ylab("") +
             xlab("") +
             stat_summary(aes(group=1), fun.y=mean, geom="line", colour="red")

grid.arrange(boreLim, hitLim, nrow = 1)
```


<font size=3>
.left[Note: 4W = 4_weeks]
</font>
---
class: middle
### The <font color="red">sample</font> means: $\hat\mu_{E16}, \; \hat\mu_{P2}, \; \hat\mu_{P6}, \; \hat\mu_{P10}, \; \hat\mu_{4W}$
``` {r}
with(devDat, tapply(gExp, list(devStage, gene), mean))
```
<center>
```{r, echo=FALSE, fig.height= 3, dev='svg'}
grid.arrange(boreLim, hitLim, nrow = 1)
```
---
class: middle
## "theHit" with significant time ("treatment") effect

``` {r,echo=FALSE,results="hide"}
means.dev <- as.data.frame(devDat %>% subset(gene=="theHit") %>% group_by(devStage) %>% summarize(cellMeans=mean(gExp)))
means.dev %>% mutate(txEffects=cellMeans-cellMeans[1])
```
![](L6_ANOVA_files/devStage.png)

---
class: middle
## "theHit" with significant time ("treatment") effect

### Can you guess the size of the $X$ matrix??
> How many dummy variables do we need?

![](L6_ANOVA_files/devStage.png)

---
class: middle
## "theHit" with significant time ("treatment") effect

``` {r,echo=FALSE,highlight.output = c(3:6)}
means.dev <- as.data.frame(devDat %>% subset(gene=="theHit") %>% group_by(devStage) %>% summarize(cellMeans=mean(gExp)))
means.dev %>% mutate(txEffects=cellMeans-cellMeans[1])
```

### We need 4 dummy variables to estimate and test 4 time differences: 
> $x_{P2}$: P2 vs E16, 
> $x_{P6}$: P6 vs E16,
> $x_{P10}$: P10 vs E16, 
> $x_{4W}$: 4W vs E16)

--

### Mathematically:

$$Y_{ij}=\theta+\tau_{P2} \times x_{ijP2}+\tau_{P6} \times x_{ijP6}+\tau_{P10} \times x_{ijP10}+\tau_{4W} \times x_{ij4W}+\varepsilon_{ij}$$
<font size=3>
*Notation*: $x_{ijk}$, where $i$ is an index for the observation, $j$ for the level of `devStage`, and $k$ for the name of the dummy variable
---
class: middle
### Under the hood, R creates a numeric $X$:

```{r,tidy=TRUE, tidy.opts=list(width.cutoff=60)}
X.matrix<-data.frame(X=model.matrix(gExp ~ devStage, irs4Dat),
                devStage=irs4Dat$devStage)
```

```{r,echo=F}
colnames(X.matrix)[2:6] <- c("X.dStP2", "X.dStP6","X.dStP10","X.dS4W","dS")
head(X.matrix,16)
```
<font size=3>
Note: column names changed and  first 16 rows displayed to fit output in the page (code hidden) 
---
class: middle

```{r,highlight.output = c(2)}
summary(lm(gExp~devStage,subset(devDat,gene=="theHit")))$coeff

means.dev %>% mutate(txEffects=cellMeans-cellMeans[1])
```

.pull-left[
<font size=5>
**Estimate**: $\hat\theta=\hat\mu_{E16}=\bar{Y}_{.E16}$

$H_0: \theta=0$ or 

$H_0: \mu_{E16}=0$
]

.pull-right[
<br>

> we are not usually interested in testing this hypothesis
]

---
class: middle

```{r,highlight.output = c(3)}
summary(lm(gExp~devStage,subset(devDat,gene=="theHit")))$coeff

means.dev %>% mutate(txEffects=cellMeans-cellMeans[1])
```

.pull-left[
<font size=5>
**Estimate**: $\hat\tau_{P2}=\hat\mu_{P2}-\hat\mu_{E16}=\bar{Y}_{.P2}-\bar{Y}_{.E16}$

$H_0: \tau_{P2}=0$ or

$H_0: \mu_{P2}=\mu_{E16}$
]
.pull-right[
> we *are* usually interested in testing this hypothesis: first 2 days after birth
]
---
class: middle

```{r,highlight.output = c(6)}
summary(lm(gExp~devStage,subset(devDat,gene=="theHit")))$coeff

means.dev %>% mutate(txEffects=cellMeans-cellMeans[1])
```

.pull-left[
<font size=5>
**Estimate**: $\hat\tau_{4W}=\hat\mu_{4W}-\hat\mu_{E16}=\bar{Y}_{.4W}-\bar{Y}_{.E16}$

$H_0: \tau_{4W}=0$ or

$H_0: \mu_{4W}=\mu_{E16}$
]

.pull-right[
<br>
> we *are* usually interested in testing this hypothesis: 4 weeks after birth
]
---
class: middle

```{r, echo=FALSE, fig.height= 3, dev='svg'}
hitLim
```
![](L6_ANOVA_files/same_se.png)

---
class: middle
<big>
  <font size = 5>$$Y = X \alpha + \varepsilon$$</font>
    $$\alpha = (\theta, \tau_{P2}, \tau_{P6}, \tau_{P10}, \tau_{4W})$$
      
### We generally test two types of null hypotheses:
      
.pull-left[
<center>
$$H_0: \tau_j = 0$$
vs
$$H_0: \tau_j \neq 0$$
for each *j* <font color="red">individually</font>
          
e.g., Is gene A differencially expressed 2 days after birth?
$$H_0: \tau_{P2}=0$$
          ]
    
.pull-right[
<center>

$$H_0: \tau_j = 0$$
        vs
$$H_0: \tau_j \neq 0$$
for all *j* <font color="red">at the same time</font>
        
e.g., Is gene A significantly affected by time (`devStage`)?
        
$$H_0: \tau_{P2}=\tau_{P6}=\tau_{P10}=\tau_{4W}=0$$
        ]
---
class: middle
### Two types of null hypotheses in R:

```{r,echo=FALSE,out.height="530px"}
    knitr::include_graphics("L6_ANOVA_files/one_more_H0.png")
```
    
---
class: middle
    
## *F*-test and overall significance of one or more covariates
<big>
  - the *t*-test in linear regression allows us to test single hypotheses:
      $$H_0 : \tau_i = 0$$
      $$H_A : \tau_j \neq 0$$
  - but we often like to test multiple hypotheses *simultaneously*: 
      $$H_0 : \tau_{P2} = \tau_{P6} = \tau_{P10} = \tau_{4W}=0\textrm{ [AND statement]}$$
      $$H_A : \tau_i \neq 0 \textrm{ for some i [OR statement]}$$ the *F*-test allows us to test such compound tests
---
class: middle
## To conclude

### - we can use different parametrizations to write statistical models
From <font color=red>**cell-means**</font> - $\mu_j$:  $Y_{ij} = \mu_j + \varepsilon_{ij};\; \; \varepsilon_{ij} \sim G; \; \;E[\varepsilon_{ij}] = 0;$

to <font color = "red">**reference-treatment effect**</font> - $(\theta,\tau_j)$: (used by default by `lm`)
$$Y_{ij} = \theta+\tau_j + \varepsilon_{ij};\; \; \tau_1=0, \; \; \varepsilon_{ij} \sim G; \; \;E[\varepsilon_{ij}] = 0;$$

### - we can compare group means  (2 or more) using a linear model
  - <font color=red>**dummy variables**</font> (e.g., $x_{ijP2}$) to model the levels of a qualitative explanatory variables
  $$Y_{ij}=\theta+\tau_{P2} \times x_{ijP2}+\tau_{P6} \times x_{ijP6}+\tau_{P10} \times x_{ijP10}+\tau_{4W} \times x_{ij4W}+\varepsilon_{ij}$$
  - qualitative variables need to be set as "factors" in the data --> R creates the dummy variables

---
class:middle

### - we can write a linear model using matrix notation: 
<font size =5>

$$Y = X \alpha + \varepsilon$$

### - <font color = "red">Linear model</font> can include **quantitative & qualitative covariates**. 

<center>
```{r,echo=FALSE,out.height="250px"}
knitr::include_graphics("L6_ANOVA_files/LM_vbles.png")
```
</center>

### - distinguish between single and joint hypotheses:
  - $t$-tests vs $F$-tests
  