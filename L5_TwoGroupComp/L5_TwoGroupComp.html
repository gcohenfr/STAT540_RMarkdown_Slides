<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Lecture 5 – Two Group Comparisons</title>
    <meta charset="utf-8" />
    <meta name="author" content="Keegan Korthauer" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <font color=red>Lecture 5 – Two Group Comparisons</font>
## STAT/BIOF/GSAT 540: Statistical Methods for High Dimensional Biology
### Keegan Korthauer
### 2020/01/20 <br><br> <font size=5> Slides by: Gabriela Cohen Freue with contributions from Jenny Bryan, Sara Mostafavi, and Keegan Korthauer </font>

---

class: middle
layout: true

&lt;big&gt;
---

&lt;style&gt;
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
&lt;/style&gt;

# &lt;center&gt; Central dogma of statistics

&lt;center&gt;
&lt;img src="L5_TwoGroupComp_files/statInference.png" alt="Central dogma of statistics" width="700"/&gt;&lt;br&gt;&lt;small&gt;&lt;a href="https://www.gs.washington.edu/academics/courses/akey/56008/lecture/lecture1.pdf" style="color:grey;"&gt;Image source: Josh Akey's Lecture notes&lt;/a&gt;&lt;/small&gt;&lt;/center&gt;

&lt;br&gt;We want to understand a **population** (e.g., gene behaviour) but we can only study a &lt;font color=red&gt;**random sample**&lt;/font&gt; from it. 
---

# &lt;center&gt; Book and online resources

* [Modern Statistics for Modern Biology](https://www.huber.embl.de/msmb/) by Susan Holmes and Wolfgang Huber, 2019 (free online book)

* [Data Analysis for the Life Sciences](http://genomicsclass.github.io/book/) by Rafael Irizarry and Michael Love, 2015 (free online book)

* [Practical Regression and Anova using R](http://www.biostat.jhsph.edu/~iruczins/teaching/jf/faraway.html) by Julian Faraway, 2002 (free online book)

* Linear Models with R by Julian Faraway, Chapman &amp; Hall/CRC Texts in Statistical Science, 2004 

---

# &lt;center&gt; Hypothesis Testing in Genomics

[![Akimoto et al. (2006)](L5_TwoGroupComp_files/paperRef.PNG)](https://doi.org/10.1073/pnas.0508214103)

&lt;big&gt;

&lt;img width="300" style="padding:0 px; float:right" src="L5_TwoGroupComp_files/cellTypes.png"&gt;
- Retina presents a model system for investigating **regulatory networks** underlying neuronal differentiation.

- **Nrl** transcription factor is known to be important for Rod development
--

- &lt;font color="red"&gt;**What happens if you delete Nrl?**

---

# &lt;center&gt; Why a Hypothesis Test?
&lt;big&gt;
From the [Akimoto et al. (2006) paper](https://doi.org/10.1073/pnas.0508214103 ): 
&gt;"we hypothesized that Nrl is the ideal transcription factor to gain insights into gene expression changes ..."

&lt;div class = "blue"&gt;
&lt;b&gt;Biological question:&lt;/b&gt; Is the expression level of gene &lt;ii&gt;A&lt;/i&gt; affected by knockout of the &lt;i&gt;Nrl&lt;/i&gt; gene?
&lt;/div&gt;
--

### We can use &lt;font color=red&gt;statistical inference&lt;/font&gt; to answer this biological question!

---

# &lt;center&gt; Statistical inference

&lt;big&gt;
**Statistical inference**:

We observe and study a &lt;font color=red&gt;random sample&lt;/font&gt; to make conclusions about a population (e.g., random sample of gene expressions from mice)

&lt;img width="350" style="padding:0 px; float:right" src="L5_TwoGroupComp_files/NrlKOmouse.png"&gt;

**Experimental design**:
* 4 developmental stages
* 2 genotypes: Wild type (WT), Nrl Knockout (NrlKO)
* 3-4 replicates for each combination


---

&lt;big&gt;
Let's take a look at 2 genes as an example: **Irs4 and Nrl**

&lt;div class = "blue"&gt;
&lt;b&gt;Biological question:&lt;/b&gt; Are these genes truly different in NrlKO compared to WT?
&lt;/div&gt;

We can't answer this question in general. We can *only* study these genes in collected data:



&lt;img src="L5_TwoGroupComp_files/figure-html/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;

--

### &lt;center&gt;&lt;font color="red"&gt; We only observe a random sample of gene expressions.&lt;/font&gt;


---

# &lt;center&gt; Statistical Hypothesis
&lt;big&gt;
** Experimental design:**
  - 2 conditions: WT *vs* NrlKO
  - random sample: we observe the expression of many genes in all mice

** Biological hypothesis:** for *some* genes, the expression levels are different between conditions.

** Statistical hypotheses:** (for each gene `\(g=1,...,G\)`)
  - H&lt;sub&gt;0&lt;/sub&gt; (null hypothesis): the expression level of gene `\(g\)` is the *same* in both conditions.
  -  &lt;span style="color:blue"&gt;H&lt;sub&gt;A&lt;/sub&gt; (alternative hypothesis): the expression level of gene `\(g\)` is *different* between conditions.&lt;/span&gt;
---


## Notation 

### Random variables and estimates (we can observe):

`\(Y_i\)` : expression of gene `\(g\)` in the WT sample `\(i\)`

`\(Z_i\)`: expression of gene `\(g\)` in NrlKO sample `\(i\)` 

`\(Y_1, Y_2,..., Y_{n_Y}\)` : a &lt;font color="red"&gt;random sample&lt;/font&gt; of size `\(n_Y\)`

`\(\bar{Y}=\frac{\sum_{i=1}^{n_Y}Y_i}{n_Y}\)`: sample mean of gene `\(g\)` expression from WT mice

&gt; &lt;small&gt; note the different meanings of the word "sample" 

### Population parameters (unknown/unobservable):

`\(\mu_Y = E[Y]\)` : the (population) expected expression of gene `\(g\)` in WT mice
---

&lt;big&gt;

## Is there **enough** evidence in the data to reject H&lt;sub&gt;0&lt;/sub&gt;? &lt;/font&gt;

&lt;big&gt;
`\(\color{red}{H_0: \mu_Y = \mu_Z}\)`

&lt;img src="L5_TwoGroupComp_files/figure-html/unnamed-chunk-3-1.svg" style="display: block; margin: auto;" /&gt;

### **Statistical Inference**: random samples are used to learn about the population 
---

&lt;small&gt;
## What we observe: the difference between the **sample averages**: `\(\bar{Y}\)` vs `\(\bar{Z}\)`
&lt;img src="L5_TwoGroupComp_files/figure-html/unnamed-chunk-4-1.svg" style="display: block; margin: auto;" /&gt;


```r
theAvgs &lt;- with(miniDat,
                 tapply(gExp, list(gType, gene), mean))
```


```
##        Irs4    Nrl
## wt    7.766 11.244
## NrlKO 7.740  6.090
```


```r
theDiff &lt;- theAvgs["NrlKO", ] - theAvgs["wt", ]
```

```
##   Irs4    Nrl 
## -0.026 -5.155
```

---


### Is the difference between `\(\bar{Y}\)` and `\(\bar{Z}\)` informative to reject H&lt;sub&gt;0&lt;/sub&gt;?

&lt;img src="L5_TwoGroupComp_files/figure-html/unnamed-chunk-9-1.svg" style="display: block; margin: auto;" /&gt;

&lt;big&gt;
- The sample means, `\(\bar{Y}\)` vs `\(\bar{Z}\)`, by themselves are not enough to make conclusions about the population

- What is a "large" difference? "large" relative to what?
---
class: center, middle


&lt;img src="L5_TwoGroupComp_files/figure-html/unnamed-chunk-10-1.svg" style="display: block; margin: auto;" /&gt;


.pull-left[
### What can we use to interpret the size of the mean difference?
]
.pull-right[
### `$$\frac{\bar{Y}-\bar{Z}}{??}$$`
]
---
class: center, middle
### What can we use to interpret the size of the mean difference?

.pull-left[
### "large" relative to the observed variation
]
.pull-right[
### `\(\frac{\bar{Y}-\bar{Z}}{\sqrt{V(\bar{Y}-\bar{Z})}}\)`
]

![](L5_TwoGroupComp_files/figure-html/unnamed-chunk-11-1.svg)&lt;!-- --&gt;
---

## Quantifying observed variation

&lt;small&gt;

* Recall that if `\(V(Y_i)=\sigma_Y^2\)`, then `\(V(\bar{Y})=\frac{\sigma_Y^2}{n_Y}\)`

* Assume that the random variables within each group are *independent and identically distributed* (iid), and that the groups are independent. More specifically, that
  1. `\(Y_1, Y_2,..., Y_{n_Y}\)` are iid, 
  2. `\(Z_1, Z_2,..., Z_{n_Z}\)` are iid, and
  3. `\(Y_i, Z_j\)` are independent. Then, it follows that
$$ V(\bar{Z}-\bar{Y})=\frac{\sigma_Z^2}{n_Z}+\frac{\sigma_Y^2}{n_Y}$$
* If we also assume equal population variances:&lt;/big&gt; `\(\sigma_Z^2=\sigma_Y^2=\sigma^2\)`, then
`$$V(\bar{Z}-\bar{Y})=\frac{\sigma_Z^2}{n_Z}+\frac{\sigma_Y^2}{n_Y}=\sigma^2\left[\frac{1}{n_Z}+\frac{1}{n_Y}\right]$$`
### But how can we calculate population variance `\(\sigma\)` if it is **unknown**?
---


### ...using the sample variances (combined, somehow)!
&lt;img src="L5_TwoGroupComp_files/figure-html/unnamed-chunk-12-1.svg" style="display: block; margin: auto;" /&gt;


```r
theVars &lt;- with(miniDat,
                 tapply(gExp, list(gType, gene), var))
```
.pull-left[


```
##        Irs4   Nrl
## wt    0.024 1.224
## NrlKO 0.023 0.594
```
]
.pull-right[
&lt;small&gt; `$$\text{e.g., for Nrl: } \hat{\sigma}_Y^2 = S_Y^2=\frac{1}{n_Y}\sum_{i=1}^{n_Y}(Y_i-\bar{Y})^2=1.224$$`
]
---


### Plug these sample variances into your chosen formula for the variance of the difference of sample means


Assuming &lt;b&gt;equal&lt;/b&gt; variance of Y's and Z's

`$$\hat{\sigma}_{\text{pooled}}^2=S_Y^2\frac{n_Y-1}{n_Y+n_Z-2}+S_Z^2\frac{n_z-1}{n_Y+n_Z-2}$$`
`$$\hat{V}(\bar{Z_n}-\bar{Y_n})=\hat{\sigma}_{\text{pooled}}^2\left[\frac{1}{n_Y}+\frac{1}{n_Z}\right]$$`
&lt;br&gt;
Assuming &lt;b&gt;unequal&lt;/b&gt; variance of Y's and Z's
`$$\hat{V}(\bar{Z_n}-\bar{Y_n})=\hat{\sigma}_{\bar{Z}_n-\bar{Y}_n}^2=\frac{S_Y^2}{n_Y}+\frac{S_Z^2}{n_Z}$$`

&gt; Note: the 'hat' (^) is used to distinguish an 'estimate' from a 'parameter'.

---





## &lt;center&gt; The Test Statistic:  &lt;small&gt; `\(T=\frac{\bar{Z}_n-\bar{Y}_n}{\hat{\sigma}_{Z_n-Y_n}}\)`&lt;/small&gt;

&lt;small&gt;

Assuming equal variances:

```r
tstStat &lt;- theDiff / sqrt(s2Diff)
```

```
##    Irs4     Nrl 
##  -0.529 -16.795
```
Without assuming equal variances:

```r
welchStat &lt;- theDiff / sqrt(s2DiffWelch)
```

```
##    Irs4     Nrl 
##  -0.529 -16.949
```
### Can we now say that the observed differences are 'big'?

The difference is about half a standard deviation for Irs4 and ~16 standard deviations for Nrl. 

---


### The test statistic T is a &lt;font color=red&gt;random variable&lt;/font&gt; because it's based on our &lt;font color=red&gt;random sample&lt;/font&gt;.

### We need a measure of its uncertainty to determine how big T is:
&gt; ### If we were to repeat the experiment many times, what's the probability of observing a value of T **as extreme** as the one we observed?

### We need to have a probability distribution!
### However, this is unknown to us so we need to **make more assumptions**.
---


### Theory now tells us specific **null distributions** for these test statistics, depending on your assumptions.

### --&gt;  Willing to assume that F and G are normal distributions?

.pull-left[
### &lt;center&gt; 2-sample *t*-test: 
 &lt;center&gt; (equal variances)
`$$T\sim t_{n_Y+n_Z-2}$$`
]
.pull-right[
### &lt;center&gt; Welch test:
&lt;center&gt; (unequal variances)
`$$T\sim t_{&lt;something\,ugly&gt;}$$`
]

### --&gt; Unwilling to assume that F and G are normal distributions? But you feel that n&lt;sub&gt;Y&lt;/sub&gt; and n&lt;sub&gt;Z&lt;/sub&gt; are large enough?

Then the t-distributions above or even a normal distribution are decent approximations.

---


## Student's *t*-distribution
### Recall that T is a **random variable**. Under certain assumptions, we can prove that T follows a *t*-distribution.

&lt;center&gt;&lt;img src="L5_TwoGroupComp_files/stuTDist.png" width="400"&gt;

where df = degrees of freedom.
---

# &lt;center&gt; Hypothesis testing

### 1. Define a **test statistic**: 2-sample *t*-test
### 2. Compute the &lt;font color=red&gt;observed value&lt;/font&gt; for the test statistic


```r
tstStat &lt;- theDiff / sqrt(s2Diff)
```

```
##    Irs4     Nrl 
##  -0.529 -16.795
```

---

### 3. Compute the probability of seeing a test statistic at least as extreme as that observed, under the **null sampling distribution** (this is the definition of the p-value) 



```
## miniDat$gene: Irs4
## [1] 0.6002058
## ------------------------------------------------------------ 
## miniDat$gene: Nrl
## [1] 6.764663e-19
```

.pull-left[
### In other words, assuming that H&lt;sub&gt;0&lt;/sub&gt; is true: 
For Irs4, the probability of seeing a test statistic as extreme as that observed `\((t = -0.53)\)` is pretty high `\((p = 0.6)\)`.

But for Nrl, the probability of seeing a test statistic as extreme as that observed `\((t = -16.8)\)` is extremely low `\((p=6.76 \times 10^{-19})\)`

]
.pull-right[
&lt;img src="L5_TwoGroupComp_files/figure-html/unnamed-chunk-23-1.svg" style="display: block; margin: auto;" /&gt;
]

---


### 4. Make a decision about significance of results, based on a pre-specified value (alpha, significance level)


The significance level `\(\alpha\)` is usually set at 0.05. However, this value is arbitrary and usually depends on the study.

Using `\(\alpha=0.05\)`, since the p-value for the Irs4 test is greater than 0.05, we conclude that there is not &lt;font color=red&gt;*enough*&amp;nbsp; evidence &lt;/font&gt; in the data to claim that Irs4 has a differential expression in WT compared to Nrl models. 

We do not reject H&lt;sub&gt;0&lt;/sub&gt;!

---
class: center

&lt;img src="L5_TwoGroupComp_files/slide23.png" width="850"&gt;
---


## What is a p-value?

&gt; Likelihood of obtaining a test statistic at least &lt;font color=green&gt; as extreme as the one observed&lt;/font&gt;, given that the null hypothesis is true (we are making a conditional p-value statement)

## What is a p-value &lt;font color=red&gt;NOT&lt;/font&gt;?

- Not the probability that the &lt;font color=red&gt;null hypothesis is true&lt;/font&gt;

- Not the probability that the &lt;font color=red&gt;finding is a “fluke”&lt;/font&gt;

- Not the probability of &lt;font color=red&gt;falsely rejecting the null&lt;/font&gt;

- Does not &lt;font color=red&gt;indicate the size or importance&lt;/font&gt; of observed effects. 

*[Credit to Dr. Fowler, UW]*
---

&lt;big&gt;
## "Genome-wide" testing of differential expression
- In genomics, we often perform thousands of statistical tests (e.g., a *t*-test per gene)

- The distribution of p-values across all tests provide good diagnostics/insights.

- Is it uniform (should be in most experiments) and if not, is the departure from uniform expected based on biological knowledge?

--
### Different kinds of *t*-tests:
- One sample *or* **two samples**
- One-sided *or* **two sided**
- Paired *or* **unpaired**
- **Equal variance** *or* unequal variance

---


# Types of Errors in Hypothesis Testing

&lt;center&gt;![](L5_TwoGroupComp_files/hypError.png)
*[Picture from Dr. Fowler, UW]*
---


&lt;big&gt;
What if you don't wish to assume the underlying data is normally distributed AND you aren't sure your samples are large enough to invoke CLT?
&lt;/big&gt;

## What are alternatives to the *t*-test?

&lt;big&gt;
First, one could use the t test statistic but use a **bootstrap approach** to compute its p-value. We will cover this later on.

Alternatively, there are *non-parametric* tests that are available here:

- **Wilcoxon rank sum test**, aka Mann Whitney, uses ranks to test differences in population means.

- **Kolmogorov-Smirnov test** uses the empirical CDF to test differences in population cumulative distributions.

---


# Wilcoxon rank sum test

&lt;big&gt;
Rank all data, ignoring the **grouping** variable

**Test statistic** = sum of the ranks for one group (optionally, subtract the minimum possible which is `\(\frac{n_Y(n_Y+1)}{2}\)`)

(Alternative but equivalent formulation based on the number of `\(y_i, z_i\)` pairs for which `\(y_i \geq z_i\)`)

Null distribution of such statistics can be worked out or approximated.

---

&lt;center&gt;
![](L5_TwoGroupComp_files/slide32.png)

![](L5_TwoGroupComp_files/figure-html/unnamed-chunk-24-1.svg)&lt;!-- --&gt;
---


# Kolmogorov-Smirnov test (two sample)

&lt;big&gt;

**Null hypothesis**: F = G, i.e. the distributions are the same

Estimate each CDF with the empirical CDF (ECDF)

`$$\hat{F}(x)=\frac{1}{n}\sum_{i=1}^n{I[x_i\leq{x}]}$$`

**Test statistic** is the maximum of the absolute difference between the ECDFs

`$$max|\hat{F}(x)-\hat{G}(x)|$$`

Null distribution does not depend on F, G (!)
&lt;br&gt;(I'm suppressing a detail here.)

---

&lt;center&gt;
&lt;img src="L5_TwoGroupComp_files/slide34.png" width="800"&gt;

---


# Discussion and questions ...
&lt;big&gt;

What if you are unsure whether your sample size is large enough? Outliers with small samples could be problematic

Which test result should one report ... the 2-sample *t*-test, the Wilcoxon, or the KS?

Treat p-values as one type of evidence that you should incorporate with others. 

It is worrisome when methods that are equally appropriate and defensible give very different answers.

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
