<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Lecture 4 – Review of Probability and Statistics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Keegan Korthauer" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# <font color=red>Lecture 4 – Review of Probability and Statistics</font>
## STAT/BIOF/GSAT 540: Statistical Methods for High Dimensional Biology
### Keegan Korthauer
### 2020/01/15 <br><br> <font size=5> Slides by: Sara Mostafavi and Keegan Korthauer </font>

---


layout: true

&lt;big&gt;&lt;big&gt;

---

&lt;style&gt;
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
&lt;/style&gt;

#  Preview of next 6 lectures

* **2020/01/15 - Lecture 4: Stats Philosophy, Math/stat background &amp; review**

* 2020/01/20 - Lecture 5: Statistical Inference - two group comparisons 

* 2020/01/22 - Lecture 6: Statistical Inference - linear regression and ANOVA

* 2020/01/27 - Lecture 7: Statistical Inference - linear models (more than two groups, and interaction testing)

* 2020/01/29 - Lecture 8: Statistical Inference - continuous model + limma

* 2020/02/03 - Lecture 9: Statistical Inference - multiple testing

---

#  Outline for today

* Intro: Philosophy, goals, and central concepts

* Review: Random Variables, Probability Distributions, Sampling Distribution, Estimation, Inference, CLT, Hypothesis Testing

&lt;div class = "blue"&gt;
Your goals: 
  &lt;ol&gt;1. be familiar with the terminology&lt;/ol&gt;
  &lt;ol&gt;2. have a clear understanding of the concepts&lt;/ol&gt;
&lt;/div&gt;
---

class: middle
# &lt;center&gt; What is Statistics?

  
---

#  Statistics 

* The field of statistics concerns the science of **collecting, analyzing/modeling, interpreting** data and **communicating uncertainty** about the results
  - Data science and machine learning have enabled application to 'big data'
  
* Statistical and computational methods should not be used as generic "recipes" to follow `\(\rightarrow\)` non-robust science

* We aim for:
  - rigorous understanding to perform routine statistical analysis
  
  - solid foundation to follow up on specific topics
---


#  Statistical Inference
A framework for generating conclusions about a population from noisy data from a sample

&lt;center&gt;
&lt;img src="L4_StatReview_files/statInference.png" alt="Picture from Dr. Fowler, UW" width="650"/&gt;&lt;/center&gt;

&lt;small&gt;

* Language of **probability** enables us to make *predictions* and discuss *uncertainty*
* **Statistical inference** enables us to *understand* the data and make *conclusions*
* We need both to learn from data

---

#  Review: terminology &amp; basic concepts

* Random variables and their distributions

* Models, parameters, and their estimators

* Central Limit Theorem (CLT)

* Hypothesis Testing

---

#  Variables

&lt;div class = "blue"&gt;
&lt;b&gt;Variable &lt;i&gt;(noun)&lt;/i&gt;:&lt;/b&gt; an element, feature, or factor that is liable to vary or change
&lt;/div&gt;

* In statistical terminology, a **variable** is an unknown quantity that we'd like to study

* Most research questions can be formulated as 
&gt;What's the *relationship* between two or more variables?  

---

#  Random variables

&lt;div class = "blue"&gt;
&lt;b&gt;Random Variable (RV):&lt;/b&gt; A variable whose value results from the measurement of a quantity that is subject to variation (e.g. the &lt;i&gt;outcome&lt;/i&gt; an experiment)
&lt;/div&gt;

  - Examples: a coin flip, a dice throw, the expression level of gene X
  
  - An RV has a *probability distribution*
  
---
  
#  Distributions of RVs

&lt;div class = "blue"&gt;
&lt;b&gt;Probability:&lt;/b&gt; A number assigned to an outcome/event that describes the extent to which it is likely to occur
&lt;/div&gt;

  - Must satisfy certain rules (e.g. be between 0 and 1)
  
  - Represents the (long-term) *frequency* of an event
  
--

&lt;div class = "blue"&gt;
&lt;b&gt;Probability distribution:&lt;/b&gt; A mathematical function that maps outcomes/events to probabilities
&lt;/div&gt;  
  
---

#  Example experiment: Two coin tosses

.pull-left[

* **Experiment**: Toss two coins

* **Sample space**: set of all possible outcomes &lt;small&gt; `\({\normalsize S=\{TT, TH, HT, HH\}}\)` &lt;/small&gt;

* **Random Variable of interest**: number of heads



]
.pull-right[

|       | Outcome  | Number of Heads |
| :---: |:---------:| :-----------:| 
| TT    | ![](L4_StatReview_files/tails.png)![](L4_StatReview_files/tails.png) | 0 |
| TH    | ![](L4_StatReview_files/heads.png)![](L4_StatReview_files/tails.png) | 1 |
| HT    | ![](L4_StatReview_files/tails.png)![](L4_StatReview_files/heads.png) | 1 |
| HH    | ![](L4_StatReview_files/heads.png)![](L4_StatReview_files/heads.png) | 2 |

]

---

#  Assigning probability to outcomes

&lt;small&gt;

.pull-left[

* Let:

  * `\(\omega=\)` an outcome 

  * `\(X(\omega)=\)` number of heads in `\(\omega\)`
  
* Each possible outcome is associated with a probability

* **Event:** A set of outcomes that satisfy some condition

* Each realization of the RV corresponds to an **event** (e.g. `\(X(\omega)=1\)` corresponds to the outcomes `\(TH\)` and `\(HT\)` )

]

.pull-right[

|       | `\(\omega\)`  | `\(X(\omega)\)` | Probability |
| :---: |:---------:| :-----------:| :---------: |
| TT    | ![](L4_StatReview_files/tails.png)![](L4_StatReview_files/tails.png) | 0 | 0.25 |
| TH    | ![](L4_StatReview_files/heads.png)![](L4_StatReview_files/tails.png) | 1 | 0.25 |
| HT    | ![](L4_StatReview_files/tails.png)![](L4_StatReview_files/heads.png) | 1 | 0.25 |
| HH    | ![](L4_StatReview_files/heads.png)![](L4_StatReview_files/heads.png) | 2 | 0.25 |
]

---

#  Assigning probability to events

The probability distribution of the Random Variable `\(X\)` tells us how likely each event (number of heads) is to occur in the experiment


| Event | `\(x\)` | `\(P(X=x)\)` |
| :-----------:| :-----------:| :---------: |
| ![](L4_StatReview_files/tails.png)![](L4_StatReview_files/tails.png) | 0 | 0.25 |
| ![](L4_StatReview_files/heads.png)![](L4_StatReview_files/tails.png) &lt;big&gt;, ![](L4_StatReview_files/tails.png)![](L4_StatReview_files/heads.png) | 1 | 0.50 |
| ![](L4_StatReview_files/heads.png)![](L4_StatReview_files/heads.png) | 2 | 0.25 |


&lt;small&gt; Note on notation: `\(P(X=x)\)` can also be written as `\(P_X(x)\)`

---

#  Two types of random variables

* A **discrete** RV has a countable number of possible values
  
  - e.g. throwing dice, genotype measured on a SNP chip

* A **continuous** RV takes on values in an interval of numbers

  - e.g. expression level of a gene, blood glucose level, height of individuals
  
---

#  Standard Gaussian (Normal) distribution 

&lt;small&gt;

.pull-left[
* probability density function:
`$$f(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$`

* Mean `\(=\mu\)`

* Standard Deviation `\(=\sigma\)`

* For convenience, we write `\(N(\mu, \sigma^2)\)`

* When `\(\mu=0\)` and `\(\sigma=1\)`, this is the *Standard* Normal distribution `\(N(0,1)\)`
]

.pull-right[
![](L4_StatReview_files/normal.png)
]

---

#  Gaussian (Normal) distribution 

&lt;center&gt;&lt;img src="L4_StatReview_files/normal2.png" width=650&gt;

&lt;small&gt;

`$$\text{Probability density function: }f(x|\mu,\sigma^2) = \phi(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$`

---

#  Density `\(\rightarrow\)` probability requires integration

&lt;img src="L4_StatReview_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;

---

#  Statistical Inference

* The **parameter space** is the set of all possible values for the parameter

* One major goal: to "figure out" (i.e. estimate) the **parameter values**
  - i.e. *"fit the model to the data"*
  
* The model is a representation that (we hope) approximates the data and (more importantly) the population that the data were sampled from 

* We can then use this model for:
  - hypothesis testing
  - prediction 
  - simulation
 
---

#  Statistical Inference

&lt;center&gt;
&lt;img src="L4_StatReview_files/statInference.png" alt="Picture from Dr. Fowler, UW" width="700"/&gt;&lt;/center&gt;

---

#  IID

&lt;small&gt;

* A requirement (assumption) in many settings is that the data are IID: **I**ndependent and **I**dentically **D**istributed

--

* **Identically Distributed**: a set of observations (events) are from the same population (i.e. they have the same underlying probability distribution)

  - e.g. a t-test assumes that under the null, all observations come from the same normal distribution

--

* **Independent**: all samples satisfy the condition `$$P(A,B) = P(A)P(B)$$` where `\(A\)` and `\(B\)` are events

  - i.e. the joint probability is the product of the individual event probabilities
  - The above statement is for two events, but the same definition applies for any number of events
(without loss of generality for any number of events)

---

#  Violations of independence

* Experimental design is in part about trying to avoid unwanted dependence

* Example of design with violation of independence assumption: 

&gt; Height measurements of individuals sampled from *related* females in a tall family

---

#  Parameters of the normal distribution 
.pull-left[
&lt;center&gt;&lt;img src="L4_StatReview_files/normal2.png" width=650&gt;
]

.pull-right[
`$$f(x|\mu,\sigma^2) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$`
* Mean `\(=\mu\)`

* Standard Deviation `\(=\sigma\)`

* For convenience, we write `\(N(\mu, \sigma^2)\)`

* Population parameters are unknown

]

---

#  Parameter estimation

&lt;big&gt;

* **Estimator**: A function (or rule) used to estimate a parameter of interest

* **Estimate**: A particular realization (value) of an estimator

---

#  Estimators for normally distributed data

* If we are given a sample of `\(n\)` observations from a normally distributed population, how do we estimate the parameter values `\(\mu\)` and `\(\sigma\)`?

* Recall `\(\mu\)` is the mean and `\(\sigma\)` the standard deviation of the distribution

--

`$$\hat{\mu} = \bar{x} = \frac{x_1 + x_2 + ... + x_n}{n} =  \frac{1}{n} \sum_{i=1}^n x_i$$`

`$$\hat{\sigma} = s = \sqrt{\frac{\sum_{i=1}^n(x_i - \bar{x})^2}{n-1}}$$`


---

#  Estimators vs Parameters 

|   | Estimators | Parameters |  
| ---- | :---------: | :----------: |
| Summarize | Sample | Population (ground truth) |
| Value | Computed from data | Unknown |
| Notation | `\(\hat{\theta}\)` | `\(\theta\)` |

---

&lt;small&gt;

#  Normal **Mean**: Estimator vs Parameter

|   | Estimator | Parameter |  
| ---- | :---------: | :----------: |
| Summarizes |  Sample/data | Population (ground truth) |
| Value | `\(\bar{x}=\frac{1}{n} \sum_{i=1}^n x_i\)` | Unknown |
| Notation | `\(\hat{\mu}\)` | `\(\mu\)` |

--

#  Normal **Standard Deviation**: Estimator vs Parameter

|   | Estimator | Parameter |  
| ---- | :---------: | :----------: |
| Summarizes |  Sample/data | Population (ground truth) |
| Value | `\(s=\sqrt{\frac{\sum_{i=1}^n(x_i - \bar{x})^2}{n-1}}\)` | Unknown |
| Notation | `\(\hat{\sigma}\)` | `\(\sigma\)` |

---

# Estimator for normally distributed data

* Let's say we collected a **sample** from a population we assume to be normal 

* We estimate the mean `\(\large \hat{\mu}=\bar{x}\)`

* How good is the estimate?

* The answer depends on:

--

  - sample size
  
  - variability of the population 

---

# Sampling distribution

* **Statistic**: any quantity computed from values in a sample 

* Any function (or statistic) of a sample (data) is a random variable

* Thus, any statistic (because it is random) has a probability distribution function `\(\rightarrow\)` specifically, we call this the *sampling distribution*

* Example: the sampling distribution of the mean

---

# Sampling distribution of the mean

The sample mean `\(\large \bar{x}\)` is an RV, so it has associated probability or sampling distribution

&lt;center&gt;
&lt;img src="L4_StatReview_files/samplingdist.png" width=750&gt;
&lt;/center&gt;

&lt;small&gt;&lt;small&gt;&lt;a href="http://www.incertitudes.fr/book.pdf" style="color:grey;"&gt;Image source: incertitudes.fr/book.pdf&lt;/a&gt;

---

# Central Limit Theorem (CLT)

&lt;small&gt;
By the *Central Limit Theorem (CLT)*, we know that the sampling distribution of the mean is Normal:
* with mean `\(\mu_{\bar{X}} = \mu\)` and standard deviation `\(\sigma_{\bar{X}} = \frac{\sigma}{\sqrt{n}}\)`

&lt;center&gt;
&lt;img src="L4_StatReview_files/clt.png" width=725&gt;
&lt;/center&gt;

&lt;small&gt;&lt;small&gt;&lt;a href="http://www.incertitudes.fr/book.pdf" style="color:grey;"&gt;Image source: incertitudes.fr/book.pdf&lt;/a&gt;

---

# &lt;center&gt; ⚠️ Standard deviation vs Standard error ⚠️

&lt;big&gt;

* The sampling distribution of the mean (by CLT): `$$\bar{X} \sim N(\mu, \frac{\sigma^2}{n})$$`

--

* The *standard error* of the mean is `\(\frac{\sigma}{\sqrt{n}}\)`

--

* The *standard deviation* of `\(X\)` is `\(\sigma\)`

---

class: middle

## Estimation of parameters of the sampling distribution of the mean

Just as we estimated `\(\mu\)` and `\(\sigma\)` before, we can estimate `\(\mu_{\bar{X}}\)` and `\(\sigma_{\bar{X}}\)`

  - `\(\hat{\mu}_{\bar{X}} = \hat{\mu} = \bar{x}\)`
  
  - `\(\hat{\sigma}_{\bar{X}} = \frac{\hat{\sigma}}{\sqrt{n}} = \frac{s}{\sqrt{n}}\)`

---

# Standard error of the mean

`$$\large\hat{\sigma}_{\bar{X}} = \frac{\hat{\sigma}}{\sqrt{n}} = \frac{s}{\sqrt{n}}$$`
* The standard error (SE) of the mean reflects uncertainty about the value of the population mean `\(\large\mu\)`

* The CLT assumes a 'large enough' sample: 

  - when the sample size is ~30 or more, the normal distribution is a good approximation for the sampling distribution of the mean
  
  - for smaller samples, the SE `\(\large\frac{s}{\sqrt{n}}\)` is an underestimate 
  
---

# CLT applies to any population (regardless of distribution)

&lt;small&gt;

Let `\(\normalsize X_1, X_2, ..., X_n\)` be a random sample from a population with a non-normal distribution. If the sample size `\(\normalsize n\)` is sufficiently large, then the sampling distribution of the mean will be approximately normal: `\(\normalsize \bar{X} \sim N(\mu, \frac{\sigma^2}{n})\)`

&lt;center&gt;&lt;img src="L4_StatReview_files/clt2.png" width=650&gt;&lt;/center&gt;&lt;br&gt;&lt;small&gt;&lt;small&gt;&lt;a href="https://saylordotorg.github.io/text_introductory-statistics/s10-02-the-sampling-distribution-of-t.html" style="color:grey;"&gt;Image source: saylordotorg.github.io/text_introductory-statistics&lt;/a&gt;

---

# Illustration (n = 3)
&lt;small&gt;

.pull-left[
&lt;img src="L4_StatReview_files/figure-html/unnamed-chunk-2-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="L4_StatReview_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;
]

On right: dashed pink line is `\(N(\mu, \sigma^2/n)\)`
---

# Illustration (n = 10)
&lt;small&gt;

.pull-left[
&lt;img src="L4_StatReview_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="L4_StatReview_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;
]

On right: dashed pink line is `\(N(\mu, \sigma^2/n)\)`

---

# Illustration (n = 30)
&lt;small&gt;

.pull-left[
&lt;img src="L4_StatReview_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="L4_StatReview_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;
]


On right: dashed pink line is `\(N(\mu, \sigma^2/n)\)`

---

# Illustration (n = 100)
&lt;small&gt;

.pull-left[
&lt;img src="L4_StatReview_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="L4_StatReview_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;
]


On right: dashed pink line is `\(N(\mu, \sigma^2/n)\)`

---

# Hypothesis Testing


* **Hypothesis:** A *testable (falsifiable)* idea for explaining a phenomenon

* **Statistical hypothesis:** A hypothesis that is testable on the basis of observing a process that is modeled via a set of random variables

* **Hypothesis Testing:** A formal procedure for determining whether to *accept* or *reject* a statistical hypothesis 

* Requires comparing two hypotheses:
  
  - `\(H_0\)`: null hypothesis
  
  - `\(H_A\)` or `\(H_1\)`: alternative hypothesis

---

# Hypothesis Testing: motivating example


* The expression level of gene `\(\normalsize g\)` is measured in `\(\normalsize n\)` patients with disease (e.g. cancer), and `\(\normalsize m\)` healthy (control) individuals:
  - `\(\normalsize z_1, z_2, ..., z_n\)` and `\(\normalsize y_1, y_2, ..., y_m\)`

* Is gene `\(\normalsize g\)` is differentially expressed in cancer vs healthy samples?
  - `\(\normalsize H_0: \mu_Z = \mu_Y\)`
  - `\(\normalsize H_A: \mu_Z \neq \mu_Y\)`

* In this setting, hypothesis testing allows us to determine whether observed differences between groups in our data are *significant*


---

# Steps in Hypothesis Testing

&lt;big&gt; 

1. Formulate your hypothesis as a statistical hypothesis

2. Define a test statistic (RV) that corresponds to the question. You typically know the expected distribution of the test statistic *under the null*

3. Compute the p-value associated with the observed test statistic under the null distribution `\(\normalsize p(t | H_0)\)`

---

# Motivating example (cancer vs healthy gene expression)

![](L4_StatReview_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;

---


# Motivating example (cancer vs healthy gene expression)

![](L4_StatReview_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;

---

# Motivating example (cancer vs healthy gene expression)

![](L4_StatReview_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;

--

All three samples drawn from iid Normal distributions with equal variance and `\(\mu_Z-\mu_Y=1\)`

---

# Is there a **significant** difference between the two means?

![](L4_StatReview_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;

* Mean difference needs to be put into context of the *spread (standard deviation)* 

* Also depends on the sample size 

---

# t-statistic / t-test

* Measures difference in means, adjusted for spread/standard deviation:

`$$\normalsize t=\frac{\bar{z}-\bar{y}}{SE_{\bar{z}-\bar{y}}}$$`
    for `\(z_1, z_2, ..., z_n\)` expression measurements in healthy samples and `\(y_1, y_2, ..., y_m\)` cancer samples

* Standard error estimate for the difference in means:

`$$SE_{\bar{z}-\bar{y}} = s_p \sqrt{\frac{1}{n} + \frac{1}{m}} \text{ , where  } s_p^2 = \frac{s^2_z + s^2_y}{(n-1) + (m-1)}$$`
---

# t-test

* From the theory, we know the distribution of our test statistic, if we are willing to make some assumptions


* If we assume:

  - Z and Y are normally distributed
  
  - Z and Y have equal variance
  
Then our t-statistic follows a t distribution with m+n-2 degrees of freedom `$$t \sim t_{n+m-2}$$`

---

# t distribution

.pull-left[
&lt;img src="L4_StatReview_files/tdist.png" width=600&gt;
]

.pull-right[
* t statistic value tells us how extreme our observed data is relative to the null 

* obtain p-value by computing area to the left and/or right of the t statistic (one-sided vs two-sided)

]

---

# Summary

* Random variables are variables that have an associated probability distribution 

* Any statistic of sampled data is an RV, and hence has an associated probability distribution

* The CLT gives us the sampling distribution of the mean 

* Hypothesis testing gives us a framework to assess a statistical hypothesis under the null







    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
